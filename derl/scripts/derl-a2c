#!/usr/bin/env python
"""
Script to run Advantage Actor Critic training.
"""
# pylint: disable=invalid-name
import argparse
import tensorflow as tf
from tqdm import tqdm
import derl
tf.enable_eager_execution()


def get_parser(base_parser=None):
  """ Returns A2C argument parser. """
  if base_parser is None:
    base_parser = argparse.ArgumentParser()
  base_parser.add_argument("--env-id", required=True)
  base_parser.add_argument("--logdir", required=True)
  base_parser.add_argument("--log-period", type=int, default=1)
  base_parser.add_argument("--nenvs", type=int, default=8)
  base_parser.add_argument("--num-train-steps", type=float, default=10e6)
  base_parser.add_argument("--num-runner-steps", type=int, default=5)
  base_parser.add_argument("--gamma", type=float, default=0.99)
  base_parser.add_argument("--lambda_", type=float, default=0.95)
  base_parser.add_argument("--lr", type=float, default=7e-4)
  base_parser.add_argument("--optimizer-decay", type=float, default=0.99)
  base_parser.add_argument("--optimizer-epsilon", type=float, default=1e-5)
  base_parser.add_argument("--value-loss-coef", type=float, default=0.25)
  base_parser.add_argument("--entropy-coef", type=float, default=0.01)
  base_parser.add_argument("--max-grad-norm", type=float, default=0.5)
  return base_parser


def main():
  """ Runs a2c training. """
  args = get_parser().parse_args()
  env = derl.env.nature_dqn_env(args.env_id, args.nenvs)
  policy = derl.ActorCriticPolicy(derl.NatureDQNModel([env.action_space.n, 1]))
  runner = derl.EnvRunner(env, policy, args.num_runner_steps,
                          transforms=[derl.GAE(policy, gamma=args.gamma,
                                               lambda_=args.lambda_),
                                      derl.MergeTimeBatch()])
  lr = derl.train.linear_anneal("lr", args.lr, args.num_train_steps,
                                step_var=runner.step_var)
  optimizer = tf.train.RMSPropOptimizer(lr, decay=args.optimizer_decay,
                                        epsilon=args.optimizer_epsilon)
  a2c = derl.A2C(policy, optimizer,
                 value_loss_coef=args.value_loss_coef,
                 entropy_coef=args.entropy_coef,
                 max_grad_norm=args.max_grad_norm)

  summary_writer = tf.contrib.summary.create_file_writer(args.logdir)
  summary_writer.set_as_default()
  pbar = tqdm(total=args.num_train_steps)
  with tf.contrib.summary.record_summaries_every_n_global_steps(
      args.log_period):
    while int(runner.step_var) < args.num_train_steps:
      pbar.update(int(runner.step_var) -  pbar.n)
      trajectory = runner.get_next()
      a2c.step(trajectory)

if __name__ == "__main__":
  main()
